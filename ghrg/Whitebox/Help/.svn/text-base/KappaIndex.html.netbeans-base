<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
  <head>
    <meta content="text/html; charset=iso-8859-1" http-equiv="content-type">
      <title>Whitebox GAT Help</title>
      <link rel="stylesheet" type="text/css" href="Help.css">
  </head>
  <body>
    
<h1>Kappa index of agreement</h1>

<p>This tool calculates the Kappa index of agreement (KIA) for two classified raster images. The user must 
specify the names of the two input images and the output html file. The output file will be displayed 
automatically but can also be displayed in afterwards in any web browser.. The input images must be of a
categorial data type, i.e. contain classes. As a measure of overall classification accuracy, the KIA is 
more robust than the percent agreement calculation since it takes into account the agreement occurring by 
random chance. In addition to the KIA, the tool will output the producer's and user's accuracy, the 
overall accuracy, and the error matrix. The KIA is often used as a means of assessing the accuracy of an 
image classification analysis.</p>

<br><h2 class="SeeAlso">See Also:</h2>
<ul>
<li>None</li>
</ul>
<br><h2 class="SeeAlso">Credits:</h2>
<ul>
<li><a href="mailto:jlindsay@uoguelph.ca">John Lindsay</a> (2012)</li>
</ul>
</body>
</html>
